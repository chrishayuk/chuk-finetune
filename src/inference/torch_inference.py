# src/inference/torch_inference.py
import time
import threading
from transformers import TextIteratorStreamer

from model_utils import load_model_and_tokenizer
from inference.stats_collector import StatsCollector
from inference.chat_template import build_chat_prompt
from inference.text_utils import final_cleanup
from inference.stream_handler import StreamHandler

def prepare_prompt_inputs(tokeniser, system_prompt: str, user_messages: list,
                          assistant_messages: list, device, stats: StatsCollector):
    """
    Builds and encodes the conversation prompt for the model.

    The function uses the chat template to construct a complete prompt that
    includes the system, user, and assistant messages. It then encodes this
    prompt into the format required by the model and records the prompt statistics.
    """
    # Build the prompt text from the conversation components
    prompt_text = build_chat_prompt(
        tokeniser=tokeniser,
        system_prompt=system_prompt,
        user_messages=user_messages,
        assistant_messages=assistant_messages,
        add_generation_prompt=True
    )

    # Encode the prompt text and record the time taken for encoding
    encode_start = time.time()
    model_inputs = tokeniser([prompt_text], return_tensors="pt").to(device)
    encode_end = time.time()

    # Record the token count and the elapsed time for prompt encoding
    prompt_token_count = model_inputs.input_ids.shape[1]
    stats.record_prompt_stats(
        token_count=prompt_token_count,
        elapsed_time=(encode_end - encode_start)
    )
    # Reset the peak memory stats prior to generation
    stats.reset_peak_memory_stats()

    return model_inputs

def stream_generated_text(model, model_inputs, tokeniser, device, stats, max_new_tokens: int, known_lines: set):
    """
    Streams tokens generated by the model in chunks, filtering out repeated lines.

    The function initialises a streaming mechanism using TextIteratorStreamer, starts the
    generation in a separate thread, and processes each incoming chunk. It also records the
    generation statistics and returns the complete raw text generated.
    """
    # Create a streamer to handle output tokens, skipping special tokens
    streamer = TextIteratorStreamer(tokeniser, skip_special_tokens=True)
    gen_token_count = 0

    # Create a StreamHandler to process and print each chunk of text as it arrives
    stream_handler = StreamHandler(known_lines)

    def _generate_thread():
        # Generate tokens in a separate thread to allow streaming
        model.generate(
            **model_inputs,
            max_new_tokens=max_new_tokens,
            streamer=streamer
        )

    # Start the generation thread
    generation_thread = threading.Thread(target=_generate_thread)
    generation_start = time.time()
    generation_thread.start()

    # Process each chunk as it is streamed by the model
    for raw_chunk in streamer:
        # process chunk
        stream_handler.process_chunk(raw_chunk)

        # Count the tokens generated in this chunk (excluding any special tokens)
        gen_token_count += len(tokeniser.encode(raw_chunk, add_special_tokens=False))

    # Wait for the generation thread to finish
    generation_thread.join()
    generation_end = time.time()

    print()  # Print a final newline for neat output

    # Record the peak memory usage and generation statistics
    stats.capture_peak_memory()
    stats.record_generation_stats(
        token_count=gen_token_count,
        elapsed_time=(generation_end - generation_start)
    )

    # Return the full raw text generated during the streaming process
    return stream_handler.get_full_text()

def execute_chat_generation(model_name: str, system_prompt: str, user_messages: list,
                            assistant_messages: list, max_new_tokens: int,
                            device_override: str = None):
    """
    Executes the full chat generation flow.

    This main routine performs the following:
      1. Loads the model and tokeniser.
      2. Prepares the prompt inputs from the conversation history.
      3. Streams the generated text in real time.
      4. Cleans the final output by removing repeated conversation lines.
      5. Prints a summary of the generation statistics.

    The cleaned final text is returned for use in the conversation history.
    """
    # Initialise the statistics collector
    stats = StatsCollector(None)
    
    # Load the model and tokeniser, possibly overriding the device if required
    model, tokeniser, device = load_model_and_tokenizer(model_name, device_override)
    stats.device = device

    # Prepare the inputs for the model based on the conversation prompt
    model_inputs = prepare_prompt_inputs(
        tokeniser=tokeniser,
        system_prompt=system_prompt,
        user_messages=user_messages,
        assistant_messages=assistant_messages,
        device=device,
        stats=stats
    )

    # Build a set of known lines to filter out repeated content during streaming
    known_lines = {system_prompt.strip()}
    known_lines.update(msg.strip() for msg in user_messages)
    known_lines.update(msg.strip() for msg in assistant_messages)

    # Stream the generated text from the model
    raw_text = stream_generated_text(
        model=model,
        model_inputs=model_inputs,
        tokeniser=tokeniser,
        device=device,
        stats=stats,
        max_new_tokens=max_new_tokens,
        known_lines=known_lines
    )

    # Perform a final cleanup to remove any leftover role labels or repeated lines
    final_text = final_cleanup(raw_text, known_lines)

    # Print a summary of the generation and prompt statistics
    stats.print_summary(final_text)

    # return the final text
    return final_text
