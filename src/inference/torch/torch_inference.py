# src/inference/torch_inference.py

import time
import threading
from transformers import TextIteratorStreamer

# inference imports
from inference.torch.stats_collector import StatsCollector
from inference.chat_template import build_chat_prompt
from inference.text_utils import final_cleanup
from inference.stream_handler import StreamHandler

def prepare_prompt_inputs(tokenizer, system_prompt: str, user_messages: list,
                          assistant_messages: list, device, stats: StatsCollector):
    """
    Builds and encodes the conversation prompt for the model.

    The function uses the chat template to construct a complete prompt that
    includes the system, user, and assistant messages, then encodes this
    prompt into the format required by the model. It records prompt statistics.
    """
    # Build the prompt text from the conversation components
    prompt_text = build_chat_prompt(
        tokenizer=tokenizer,
        system_prompt=system_prompt,
        user_messages=user_messages,
        assistant_messages=assistant_messages,
        add_generation_prompt=True
    )

    # Encode the prompt text and record the time taken
    encode_start = time.time()
    model_inputs = tokenizer([prompt_text], return_tensors="pt").to(device)
    encode_end = time.time()

    # Record token count and encoding time
    prompt_token_count = model_inputs.input_ids.shape[1]
    stats.record_prompt_stats(
        token_count=prompt_token_count,
        elapsed_time=(encode_end - encode_start)
    )

    # Reset peak memory stats before generation
    stats.reset_peak_memory_stats()

    return model_inputs

def stream_generated_text(model, model_inputs, tokenizer, device, stats,
                          max_new_tokens: int, known_lines: set):
    """
    Streams tokens generated by the model in chunks, filtering out repeated lines.

    The function creates a streaming mechanism using TextIteratorStreamer, starts
    the generation in a separate thread, and processes each incoming chunk.
    It also records generation statistics and returns the complete raw text.
    """
    # Create a streamer to handle output tokens, skipping special tokens
    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
    gen_token_count = 0

    # Create a StreamHandler to process each chunk of text
    stream_handler = StreamHandler(known_lines)

    def _generate_thread():
        # Generate tokens in a separate thread for streaming
        model.generate(
            **model_inputs,
            max_new_tokens=max_new_tokens,
            streamer=streamer
        )

    # Start the generation thread
    generation_thread = threading.Thread(target=_generate_thread)
    generation_start = time.time()
    generation_thread.start()

    # Process each chunk as it streams in
    for raw_chunk in streamer:
        stream_handler.process_chunk(raw_chunk)
        gen_token_count += len(tokenizer.encode(raw_chunk, add_special_tokens=False))

    # Wait for the generation to finish
    generation_thread.join()
    generation_end = time.time()

    print()  # Final newline for neat output

    # Record peak memory usage and generation stats
    stats.capture_peak_memory()
    stats.record_generation_stats(
        token_count=gen_token_count,
        elapsed_time=(generation_end - generation_start)
    )

    # Return the full raw text generated
    return stream_handler.get_full_text()

def execute_chat_generation(
    loaded_model,
    loaded_tokenizer,
    system_prompt: str,
    user_messages: list,
    assistant_messages: list,
    max_new_tokens: int
):
    """
    Executes the full chat generation flow using a *preloaded* Torch model/tokenizer.

    Steps:
      1. Accepts an already-loaded model & tokenizer.
      2. Prepares the prompt inputs from the conversation history.
      3. Streams the generated text in real time (nucleus/transformers streaming).
      4. Cleans the final output by removing repeated conversation lines.
      5. Prints a summary of the generation statistics.

    :param loaded_model: A preloaded PyTorch model instance (already on device).
    :param loaded_tokenizer: A preloaded tokenizer object.
    :param system_prompt: System message providing context or instruction.
    :param user_messages: List of user messages (strings).
    :param assistant_messages: List of previous assistant replies (strings).
    :param max_new_tokens: Maximum tokens to generate.
    :return: The final cleaned assistant response (string).
    """
    # Initialise the statistics collector
    stats = StatsCollector(None)

    # Determine the device from the preloaded model
    device = next(loaded_model.parameters()).device
    stats.device = device

    # Build and encode the prompt
    model_inputs = prepare_prompt_inputs(
        tokenizer=loaded_tokenizer,
        system_prompt=system_prompt,
        user_messages=user_messages,
        assistant_messages=assistant_messages,
        device=device,
        stats=stats
    )

    # Collect known lines for filtering out repeated content
    known_lines = {system_prompt.strip()}
    known_lines.update(msg.strip() for msg in user_messages)
    known_lines.update(msg.strip() for msg in assistant_messages)

    # Stream the generated text
    raw_text = stream_generated_text(
        model=loaded_model,
        model_inputs=model_inputs,
        tokenizer=loaded_tokenizer,
        device=device,
        stats=stats,
        max_new_tokens=max_new_tokens,
        known_lines=known_lines
    )

    # Cleanup repeated content and final role labels
    final_text = final_cleanup(raw_text, known_lines)

    # Print a summary of generation & prompt stats
    stats.print_summary(final_text)

    return final_text
